{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCrRCptIMv7M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtr3bAeRa2Gt"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/mvsa_dataset/MVSA_Single\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFj2fRQMa46n",
        "outputId": "7a6cdf30-83ba-493e-cc34-1789deb3ba3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z6cuGBMbz-W"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 3\n",
        "num_epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "54f7f823bb52442f92be323b2666b2d3",
            "7c0b115c7f5a45f69f486701b85ec58d",
            "346e6ade3e044a52ab8ef498ffea4cec",
            "61fe7ef63ee44d999fa43ee0edd4a93a",
            "4f155551fb7149f9ab7a6062194f788d",
            "583e89f516494a748c79ba2b45273f0f",
            "1a22f3c89fa1470086ced18502d5288a",
            "76dbeac688ba443fa7966047d4384e69",
            "0799a10001a848cdb6a582738bf2f87a",
            "56713b567d6b45598895d5fce85c34b7",
            "f47ffef2380640b6942a6c56cd61c76d",
            "6f868c556e1747859b06a9a325f2e9fe",
            "026c7e64b78449bdaf38a991e92cafa6",
            "9992d567c5b7443fa911f66e1f56051d",
            "4a73a6eca7144db0a4d38f99c84fcec7",
            "06b35e8dcb244e2482197b2e1f3118b5",
            "9979a8604da64a3f91e4033277c30624",
            "d0347be7eb3744b782c48afd316e17a5",
            "cbdc7f2e322d41bc9319ded17ac7b02e",
            "83c8d64f031f493aa3fc964cd08dccea",
            "a05d2309ffd84d0398f85fa18b19a1be",
            "10d578fca57d42af974641984f2844d4",
            "ff424c5d92fb4d1fb02775d536c7d072",
            "83a207a1c49247c79746977ec3f20b44",
            "0024bad4a6434cba8f54340032c3ed7e",
            "7ba3afdb66ad40f983188c2f7ebae2b3",
            "50ce6716ed6d41579a0c2e68c7c5a36f",
            "67700240e5d44f2da42edeca87355e5b",
            "6dfe929622024f25960f359251e2f23b",
            "215fe825ad454e92a3164cf3d922b5ec",
            "305a55df466e4165ae88306c2ec829c7",
            "e678a9a6ff6c4cea83ac4235bb9b9482",
            "12d8ac99693f432d8505098e7c8a2744"
          ]
        },
        "id": "qE5qCm5ab6jw",
        "outputId": "bea2715a-d501-4553-d61f-e26d739f73c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54f7f823bb52442f92be323b2666b2d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f868c556e1747859b06a9a325f2e9fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff424c5d92fb4d1fb02775d536c7d072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_path = \"/content/mvsa_dataset/MVSA_Single\"\n",
        "dataset = MVSADataset(dataset_path, tokenizer, transform)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = MCAM(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poxcV4F2c1oZ"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "your_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
        "    transforms.ToTensor(),          # Convert PIL Image to PyTorch Tensor\n",
        "    transforms.Normalize(           # Normalize using ImageNet stats\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i_b8WWSd4Qm"
      },
      "outputs": [],
      "source": [
        "label_file = \"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TbZuMHP8eH_1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check files inside your data directory\n",
        "data_path = \"/content/MVSA_Single\"  # change if your folder is different\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdouOT0qeiKC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform, tokenizer):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.samples = []\n",
        "\n",
        "        self.label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "        with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "                img_name, label_str = parts\n",
        "                label_str = label_str.split(',')[0].strip().lower()\n",
        "                if label_str in self.label_map:\n",
        "                    self.samples.append((img_name, self.label_map[label_str]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label = self.samples[idx]\n",
        "        img_path = os.path.join(self.data_dir, \"data\", img_name)\n",
        "        text_path = os.path.join(self.data_dir, \"data\", img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        # Load image and text\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        text = open(text_path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "\n",
        "        # Apply transformations and tokenizer\n",
        "        image = self.transform(image)\n",
        "        text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            text_inputs[\"input_ids\"].squeeze(0),\n",
        "            text_inputs[\"attention_mask\"].squeeze(0),\n",
        "            label\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "9loD_vpdekxd",
        "outputId": "6f5502e6-dfc6-431a-8dbe-269f61656329"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labelResultAll.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-58b1f1ba0ac9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m dataset = MVSADataset(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/MVSA_Single\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlabel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# update if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-7ae155384e9e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, label_file, transform, tokenizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"neutral\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labelResultAll.txt'"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = MVSADataset(\n",
        "    data_dir=\"/content/MVSA_Single\",\n",
        "    label_file=\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\",  # update if needed\n",
        "    transform=transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn9sX44UenBH"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MCAM(nn.Module):\n",
        "    def __init__(self, feature_dim=512, text_dim=768, num_classes=3):\n",
        "        super(MCAM, self).__init__()\n",
        "        self.image_fc = nn.Linear(3 * 224 * 224, feature_dim)\n",
        "        self.text_fc = nn.Linear(text_dim, feature_dim)\n",
        "        self.fusion = nn.Linear(feature_dim * 2, feature_dim)\n",
        "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        batch_size = image.size(0)\n",
        "        image_feat = self.image_fc(image.view(batch_size, -1))\n",
        "\n",
        "        # Fake text embedding (replace with actual BERT if fine-tuning)\n",
        "        text_feat = self.text_fc(input_ids.float())  # simple placeholder\n",
        "\n",
        "        fused = torch.cat([image_feat, text_feat], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "        out = self.classifier(fused)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEj3famLAxcn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        df = pd.read_csv(label_file)\n",
        "        df.columns = ['id', 'text_label', 'image_label']  # Rename for clarity\n",
        "\n",
        "        # Filter out rows where either image or text is missing\n",
        "        self.samples = []\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(data_dir, \"data\", f\"{row['id']}.jpg\")\n",
        "            txt_path = os.path.join(data_dir, \"data\", f\"{row['id']}.txt\")\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                self.samples.append((img_path, txt_path, row['text_label']))\n",
        "\n",
        "        self.label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, txt_path, sentiment = self.samples[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return image, inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), self.label_map[sentiment]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of9ke05te9mk"
      },
      "outputs": [],
      "source": [
        "def __getitem__(self, idx):\n",
        "    img_name, label = self.samples[idx]\n",
        "\n",
        "    # Ensure correct file extension\n",
        "    if not img_name.endswith(\".jpg\"):\n",
        "        img_name = f\"{img_name}.jpg\"\n",
        "\n",
        "    img_path = os.path.join(self.data_dir, \"data\", img_name)\n",
        "    text_path = img_path.replace(\".jpg\", \".txt\")\n",
        "\n",
        "    # Load image and text\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    text = open(text_path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "\n",
        "    # Transform and tokenize\n",
        "    image = self.transform(image)\n",
        "    text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    return (\n",
        "        image,\n",
        "        text_inputs[\"input_ids\"].squeeze(0),\n",
        "        text_inputs[\"attention_mask\"].squeeze(0),\n",
        "        label\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BI5m8sYkAzZO",
        "outputId": "a97a82a9-4f1c-478c-971d-02f3e94b8075"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labels.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7de9d3f72207>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m dataset = MVSADataset(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/MVSA_Single/MVSA_Single\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlabel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/MVSA_Single/MVSA_Single/labels.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-bdc414be24ec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, label_file, transform, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_label'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Rename for clarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labels.csv'"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define transformations\n",
        "your_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = MVSADataset(\n",
        "    data_dir=\"/content/MVSA_Single/MVSA_Single\",\n",
        "    label_file=\"/content/MVSA_Single/MVSA_Single/labels.csv\",\n",
        "    transform=your_transforms,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Check dataset size\n",
        "print(\"Total samples:\", len(dataset))\n",
        "\n",
        "# Train-validation split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KTjxrO6BFRc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Read TSV with header: ID, text_label, image_label\n",
        "        df = pd.read_csv(label_file, sep=\"\\t\")\n",
        "        df.columns = ['id', 'text_label', 'image_label']\n",
        "\n",
        "        self.samples = []\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(data_dir, \"data\", f\"{row['id']}.jpg\")\n",
        "            txt_path = os.path.join(data_dir, \"data\", f\"{row['id']}.txt\")\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                self.samples.append((img_path, txt_path, row['text_label']))\n",
        "\n",
        "        self.label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, txt_path, sentiment = self.samples[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return image, inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), self.label_map[sentiment]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnH4hdpYfVjE"
      },
      "outputs": [],
      "source": [
        "def __getitem__(self, idx):\n",
        "    img_name, label_str = self.samples[idx]\n",
        "\n",
        "    # Always add .jpg and .txt\n",
        "    img_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.jpg\")\n",
        "    text_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.txt\")\n",
        "\n",
        "    # Open image and text safely\n",
        "    try:\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Missing image: {img_path}\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Missing text: {text_path}\")\n",
        "        raise\n",
        "\n",
        "    # Transform image\n",
        "    image = self.transform(image)\n",
        "\n",
        "    # Tokenize text\n",
        "    text_inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    # Label cleanup (handle multiple sentiments like \"positive,neutral\")\n",
        "    label_str = label_str.split(\",\")[0].strip().lower()\n",
        "    label = self.label_map.get(label_str, -1)\n",
        "\n",
        "    if label == -1:\n",
        "        print(f\"Unknown label: {label_str}\")\n",
        "        raise ValueError(f\"Unknown label: {label_str}\")\n",
        "\n",
        "    return (\n",
        "        image,\n",
        "        text_inputs[\"input_ids\"].squeeze(0),\n",
        "        text_inputs[\"attention_mask\"].squeeze(0),\n",
        "        label\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yniZ2XNcBHCn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Read CSV with two columns: ID and \"text,image\"\n",
        "        df = pd.read_csv(label_file)\n",
        "        df.columns = ['id', 'combined_labels']  # Rename for clarity\n",
        "        df[['text_label', 'image_label']] = df['combined_labels'].str.split(\",\", expand=True)\n",
        "\n",
        "        self.samples = []\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(data_dir, \"data\", f\"{row['id']}.jpg\")\n",
        "            txt_path = os.path.join(data_dir, \"data\", f\"{row['id']}.txt\")\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                self.samples.append((img_path, txt_path, row['text_label']))\n",
        "\n",
        "        self.label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, txt_path, sentiment = self.samples[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return image, inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), self.label_map[sentiment]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "oF89nuGtBU4U",
        "outputId": "85482775-fb76-47ac-e8f0-0083adaca3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Main directory contents:\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-cb30ee93fa11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check files in main dataset directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Main directory contents:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/MVSA_Single/MVSA_Single\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check inside /data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check files in main dataset directory\n",
        "print(\"Main directory contents:\")\n",
        "print(os.listdir(\"/content/MVSA_Single/MVSA_Single\"))\n",
        "\n",
        "# Check inside /data directory\n",
        "print(\"\\nData directory contents (first 10 files):\")\n",
        "print(os.listdir(\"/content/MVSA_Single/MVSA_Single/data\")[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYJk0avMBo3U",
        "outputId": "fb2b9dbf-c320-43d8-cec0-49a80c0b65e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-level contents of /content:\n",
            "['.config', 'mvsa_dataset', 'kaggle.json', 'mvsasingle.zip', 'kaggle (1).json', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Start by checking the top level\n",
        "print(\"Top-level contents of /content:\")\n",
        "print(os.listdir(\"/content\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gY8KG9dBxBr",
        "outputId": "6c8d1047-5bdd-4d40-cf43-a7aac8434fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of /content/mvsa_dataset:\n",
            "['MVSA_Single']\n"
          ]
        }
      ],
      "source": [
        "print(\"Contents of /content/mvsa_dataset:\")\n",
        "print(os.listdir(\"/content/mvsa_dataset\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgrtzGLpB3Au",
        "outputId": "338fc3e7-6598-4396-86fc-a17fc2733d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of /content/mvsa_dataset/MVSA_Single:\n",
            "['data', 'labelResultAll.txt']\n"
          ]
        }
      ],
      "source": [
        "print(\"Contents of /content/mvsa_dataset/MVSA_Single:\")\n",
        "print(os.listdir(\"/content/mvsa_dataset/MVSA_Single\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "addqrX60CBZQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Read TSV (Tab-Separated Values) without header\n",
        "        df = pd.read_csv(label_file, sep=\"\\t\", header=None, names=[\"id\", \"text_label\", \"image_label\"])\n",
        "\n",
        "        self.samples = []\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(data_dir, \"data\", f\"{row['id']}.jpg\")\n",
        "            txt_path = os.path.join(data_dir, \"data\", f\"{row['id']}.txt\")\n",
        "\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    text = f.read().strip()\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image_path\": img_path,\n",
        "                    \"text\": text,\n",
        "                    \"label\": row[\"text_label\"]\n",
        "                })\n",
        "\n",
        "        self.label2idx = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Image\n",
        "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Text\n",
        "        encoded = self.tokenizer(\n",
        "            sample[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoded[\"attention_mask\"].squeeze()\n",
        "        label = self.label2idx[sample[\"label\"]]\n",
        "\n",
        "        return image, input_ids, attention_mask, torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S80_WflLCUco"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    text = f.read().strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLzQzMHCFz6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "\n",
        "        # Load labels from the correct label file path\n",
        "        label_path = label_file\n",
        "        df = pd.read_csv(label_path, sep=\"\\t\", header=None, names=[\"id\", \"text_label\", \"image_label\"])\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            img_name = f\"{row['id']}.jpg\"\n",
        "            txt_name = f\"{row['id']}.txt\"\n",
        "\n",
        "            img_path = os.path.join(data_dir, \"data\", img_name)\n",
        "            txt_path = os.path.join(data_dir, \"data\", txt_name)\n",
        "\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                # Safely read text with encoding fix\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read().strip()\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image\": img_path,\n",
        "                    \"text\": text,\n",
        "                    \"label\": row[\"text_label\"].strip().lower()  # You can also use \"image_label\"\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize text\n",
        "        text = sample[\"text\"]\n",
        "        encoded_text = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = self._label_to_int(sample[\"label\"])\n",
        "\n",
        "        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0), label\n",
        "\n",
        "    def _label_to_int(self, label):\n",
        "        mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "        return mapping.get(label, 1)  # Default to neutral if unknown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCNLm3reDiV6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "        self.missing_samples = 0  # Track missing samples\n",
        "\n",
        "        # Load labels from the correct label file path\n",
        "        label_path = label_file\n",
        "        df = pd.read_csv(label_path, sep=\"\\t\", header=None, names=[\"id\", \"text_label\", \"image_label\"])\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            img_name = f\"{row['id']}.jpg\"\n",
        "            txt_name = f\"{row['id']}.txt\"\n",
        "\n",
        "            img_path = os.path.join(data_dir, \"data\", img_name)\n",
        "            txt_path = os.path.join(data_dir, \"data\", txt_name)\n",
        "\n",
        "            # Check if both the image and text file exist\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                # Safely read text with encoding fix\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read().strip()\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image\": img_path,\n",
        "                    \"text\": text,\n",
        "                    \"label\": row[\"text_label\"].strip().lower()  # You can also use \"image_label\"\n",
        "                })\n",
        "            else:\n",
        "                self.missing_samples += 1  # Count missing samples\n",
        "\n",
        "        print(f\"Total missing samples: {self.missing_samples}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize text\n",
        "        text = sample[\"text\"]\n",
        "        encoded_text = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = self._label_to_int(sample[\"label\"])\n",
        "\n",
        "        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0), label\n",
        "\n",
        "    def _label_to_int(self, label):\n",
        "        mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "        return mapping.get(label, 1)  # Default to neutral if unknown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaQatGA_D6fS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "        self.missing_samples = 0  # Track missing samples\n",
        "        self.missing_files = []   # To log missing files\n",
        "\n",
        "        # Load labels from the correct label file path\n",
        "        label_path = label_file\n",
        "        df = pd.read_csv(label_path, sep=\"\\t\", header=None, names=[\"id\", \"text_label\", \"image_label\"])\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            img_name = f\"{row['id']}.jpg\"\n",
        "            txt_name = f\"{row['id']}.txt\"\n",
        "\n",
        "            img_path = os.path.join(data_dir, \"data\", img_name)\n",
        "            txt_path = os.path.join(data_dir, \"data\", txt_name)\n",
        "\n",
        "            # Check if both the image and text file exist\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                # Safely read text with encoding fix\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read().strip()\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image\": img_path,\n",
        "                    \"text\": text,\n",
        "                    \"label\": row[\"text_label\"].strip().lower()  # You can also use \"image_label\"\n",
        "                })\n",
        "            else:\n",
        "                self.missing_samples += 1  # Count missing samples\n",
        "                self.missing_files.append({\"image\": img_path, \"text\": txt_path})  # Log missing files\n",
        "\n",
        "        print(f\"Total missing samples: {self.missing_samples}\")\n",
        "        if self.missing_samples > 0:\n",
        "            print(f\"Missing files: {self.missing_files[:10]}\")  # Show a sample of the missing files for debugging\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize text\n",
        "        text = sample[\"text\"]\n",
        "        encoded_text = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = self._label_to_int(sample[\"label\"])\n",
        "\n",
        "        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0), label\n",
        "\n",
        "    def _label_to_int(self, label):\n",
        "        mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "        return mapping.get(label, 1)  # Default to neutral if unknown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmiC4Fc4D9LM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "        self.missing_samples = 0  # Track missing samples\n",
        "        self.missing_files = []   # To log missing files\n",
        "\n",
        "        # Load labels from the correct label file path\n",
        "        label_path = label_file\n",
        "        df = pd.read_csv(label_path, sep=\"\\t\", header=None, names=[\"id\", \"text_label\", \"image_label\"])\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            img_name = f\"{row['id']}.jpg\"\n",
        "            txt_name = f\"{row['id']}.txt\"\n",
        "\n",
        "            img_path = os.path.join(data_dir, \"data\", img_name)\n",
        "            txt_path = os.path.join(data_dir, \"data\", txt_name)\n",
        "\n",
        "            # Check if both the image and text file exist\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                # Safely read text with encoding fix\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read().strip()\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"image\": img_path,\n",
        "                    \"text\": text,\n",
        "                    \"label\": row[\"text_label\"].strip().lower()  # You can also use \"image_label\"\n",
        "                })\n",
        "            else:\n",
        "                self.missing_samples += 1  # Count missing samples\n",
        "                self.missing_files.append({\"image\": img_path, \"text\": txt_path})  # Log missing files\n",
        "\n",
        "        print(f\"Total missing samples: {self.missing_samples}\")\n",
        "        if self.missing_samples > 0:\n",
        "            print(f\"Missing files: {self.missing_files[:10]}\")  # Show a sample of the missing files for debugging\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize text\n",
        "        text = sample[\"text\"]\n",
        "        encoded_text = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = self._label_to_int(sample[\"label\"])\n",
        "\n",
        "        return image, encoded_text[\"input_ids\"].squeeze(0), encoded_text[\"attention_mask\"].squeeze(0), label\n",
        "\n",
        "    def _label_to_int(self, label):\n",
        "        mapping = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "        return mapping.get(label, 1)  # Default to neutral if unknown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTqH6d7GEulB"
      },
      "outputs": [],
      "source": [
        "class MVSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=256):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        # Read CSV or TXT file\n",
        "        with open(self.label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            name, sentiment = line.strip().split(\",\")  # Assuming name and sentiment are comma-separated\n",
        "\n",
        "            # Define paths for image and text files\n",
        "            img_path = os.path.join(self.data_dir, \"data\", f\"{name}.jpg\")\n",
        "            txt_path = os.path.join(self.data_dir, \"data\", f\"{name}.txt\")\n",
        "\n",
        "            # Skip missing pairs\n",
        "            if not os.path.exists(img_path) or not os.path.exists(txt_path):\n",
        "                continue\n",
        "\n",
        "            # If both files exist, add sample to the list\n",
        "            with open(txt_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
        "                text = txt_file.read().strip()\n",
        "\n",
        "            # Add sample dictionary to the list\n",
        "            self.samples.append({\n",
        "                'id': name,\n",
        "                'text': text,\n",
        "                'image': img_path,\n",
        "                'label': sentiment\n",
        "            })\n",
        "\n",
        "        print(f\"Total samples after cleaning: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        img = Image.open(sample['image']).convert(\"RGB\")\n",
        "        text = sample['text']\n",
        "        label = int(sample['label'])  # Assuming sentiment is an integer (0, 1, 2, etc.)\n",
        "\n",
        "        # Apply transformation if provided\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Tokenize the text if a tokenizer is provided\n",
        "        if self.tokenizer:\n",
        "            encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
        "            input_ids = encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n",
        "            attention_mask = encoding['attention_mask'].squeeze(0)  # Remove the batch dimension\n",
        "        else:\n",
        "            input_ids = None\n",
        "            attention_mask = None\n",
        "\n",
        "        return img, input_ids, attention_mask, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKhOJc-7E4bi",
        "outputId": "9f151f0c-434e-4a0f-dff2-a62e0a69c168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "dataset = MVSADataset(\n",
        "    data_dir=\"/content/mvsa_dataset/MVSA_Single\",\n",
        "    label_file=\"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\",\n",
        "    transform=transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Now you can proceed with the train/validation split and DataLoader creation as before\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHmCneDmFA15",
        "outputId": "823e082a-74a3-43b5-e879-88f3a334a4b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\ttext,image\n",
            "1\tneutral,positive\n",
            "2\tneutral,positive\n",
            "3\tneutral,positive\n",
            "4\tpositive,positive\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 5:  # Print the first 5 lines for inspection\n",
        "            print(line.strip())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieERSSbgFKHR"
      },
      "outputs": [],
      "source": [
        "class MVSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None, max_length=256):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        # Read the label file\n",
        "        with open(self.label_file, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            # Split the line into the ID and labels (text_label, image_label)\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            sample_id = parts[0]\n",
        "            text_label, image_label = parts[1].split(\",\")  # Split the labels on comma\n",
        "\n",
        "            # Define paths for image and text files\n",
        "            img_path = os.path.join(self.data_dir, \"data\", f\"{sample_id}.jpg\")\n",
        "            txt_path = os.path.join(self.data_dir, \"data\", f\"{sample_id}.txt\")\n",
        "\n",
        "            # Check if the image and text files exist\n",
        "            if not os.path.exists(img_path) or not os.path.exists(txt_path):\n",
        "                print(f\"Skipping missing pair -> {img_path} or {txt_path}\")\n",
        "                continue\n",
        "\n",
        "            # Read the text content, using 'ignore' to bypass encoding issues\n",
        "            with open(txt_path, \"r\", encoding=\"utf-8\", errors='ignore') as txt_file:\n",
        "                text = txt_file.read().strip()\n",
        "\n",
        "            # Add the sample to the list\n",
        "            self.samples.append({\n",
        "                'id': sample_id,\n",
        "                'text': text,\n",
        "                'image': img_path,\n",
        "                'text_label': text_label,\n",
        "                'image_label': image_label\n",
        "            })\n",
        "\n",
        "        print(f\"Total samples after cleaning: {len(self.samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTjGbhg5FQZ4",
        "outputId": "5ad8eddf-26dd-48ba-b247-51d904488b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping missing pair -> /content/mvsa_dataset/MVSA_Single/data/ID.jpg or /content/mvsa_dataset/MVSA_Single/data/ID.txt\n",
            "Total samples after cleaning: 4869\n"
          ]
        }
      ],
      "source": [
        "dataset = MVSADataset(\n",
        "    data_dir=\"/content/mvsa_dataset/MVSA_Single\",\n",
        "    label_file=\"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\",\n",
        "    transform=transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Now you can proceed with the train/validation split and DataLoader creation as before\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnVM2hEFuxc",
        "outputId": "b791c422-7d33-4c21-cb46-a8f25c272c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'id': '1', 'text': 'How I feel today #legday #jelly #aching #gym', 'image': '/content/mvsa_dataset/MVSA_Single/data/1.jpg', 'text_label': 'neutral', 'image_label': 'positive'}, {'id': '2', 'text': 'grattis min griskulting!!!???? va bara tvungen oki s? sch ? @ingenkommeratttrodig #pig #happybday #wow #lovely #cut', 'image': '/content/mvsa_dataset/MVSA_Single/data/2.jpg', 'text_label': 'neutral', 'image_label': 'positive'}, {'id': '3', 'text': 'RT @polynminion: The moment I found my favourite tV character. #PROFOUNDLOVE', 'image': '/content/mvsa_dataset/MVSA_Single/data/3.jpg', 'text_label': 'neutral', 'image_label': 'positive'}, {'id': '4', 'text': '#escort We have a young and energetic team and we pride ourselves on offering the highes #hoer', 'image': '/content/mvsa_dataset/MVSA_Single/data/4.jpg', 'text_label': 'positive', 'image_label': 'positive'}, {'id': '5', 'text': 'RT @chrisashaffer: Went to SSC today to be a \"movie star\" to rep for the Deaf and got to meet an energetic great guy: Mark White!', 'image': '/content/mvsa_dataset/MVSA_Single/data/5.jpg', 'text_label': 'positive', 'image_label': 'positive'}]\n"
          ]
        }
      ],
      "source": [
        "# Inspect the structure of a sample\n",
        "print(dataset.samples[:5])  # Print the first 5 samples to check their structure\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p31wZe9WflKX",
        "outputId": "7fcdd6b4-7d1a-4978-b61d-997e936c8c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total missing: 0\n"
          ]
        }
      ],
      "source": [
        "missing = 0\n",
        "for sample in dataset.samples:\n",
        "    name = sample['id']  # Access the 'id' from the sample dictionary\n",
        "    img = os.path.join(\"/content/mvsa_dataset/MVSA_Single/data\", f\"{name}.jpg\")\n",
        "    txt = os.path.join(\"/content/mvsa_dataset/MVSA_Single/data\", f\"{name}.txt\")\n",
        "    if not os.path.exists(img) or not os.path.exists(txt):\n",
        "        print(f\"Missing pair -> {img} or {txt}\")\n",
        "        missing += 1\n",
        "\n",
        "print(f\"Total missing: {missing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwy-iZ0IFtfN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoVESlw1CD_x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DilcHIJOBT7T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBXoyVMkf1th",
        "outputId": "93c24857-b429-4994-890f-29c8338d2e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images: 4869\n",
            "Number of text files: 4869\n",
            "Sample image files: ['2597.jpg', '2619.jpg', '1469.jpg', '2445.jpg', '4345.jpg']\n",
            "Sample text files: ['4.txt', '3991.txt', '2072.txt', '3752.txt', '4355.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_folder = \"/content/mvsa_dataset/MVSA_Single/data\"\n",
        "\n",
        "# List all image and text files\n",
        "img_files = [f for f in os.listdir(data_folder) if f.endswith(\".jpg\")]\n",
        "txt_files = [f for f in os.listdir(data_folder) if f.endswith(\".txt\")]\n",
        "\n",
        "print(f\"Number of images: {len(img_files)}\")\n",
        "print(f\"Number of text files: {len(txt_files)}\")\n",
        "\n",
        "# Optionally: show a few filenames\n",
        "print(\"Sample image files:\", img_files[:5])\n",
        "print(\"Sample text files:\", txt_files[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grxYHKz_gel9",
        "outputId": "a1973433-9cf0-415e-f4dd-456d2b213bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Valid pairs: 4869\n"
          ]
        }
      ],
      "source": [
        "label_file = \"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\"\n",
        "data_dir = \"/content/mvsa_dataset/MVSA_Single/data\"\n",
        "\n",
        "valid_samples = []\n",
        "\n",
        "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        name, label = parts[0], parts[1]\n",
        "        img_path = os.path.join(data_dir, f\"{name}.jpg\")\n",
        "        txt_path = os.path.join(data_dir, f\"{name}.txt\")\n",
        "\n",
        "        if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "            valid_samples.append((name, label))\n",
        "\n",
        "print(f\"✅ Valid pairs: {len(valid_samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_6smaDugxpx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content/MVSA_Single\"):\n",
        "    print(f\"{root} -> {len(files)} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-e5Kfqdg63k",
        "outputId": "ed2fa11e-771d-46a7-957f-9a58442a39b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\ttext,image\n",
            "\n",
            "1\tneutral,positive\n",
            "\n",
            "2\tneutral,positive\n",
            "\n",
            "3\tneutral,positive\n",
            "\n",
            "4\tpositive,positive\n",
            "\n",
            "5\tpositive,positive\n",
            "\n",
            "6\tpositive,positive\n",
            "\n",
            "7\tpositive,positive\n",
            "\n",
            "8\tneutral,positive\n",
            "\n",
            "9\tpositive,positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(10):\n",
        "        print(f.readline())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLbMT9eQg989"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t775IsYxhPO6",
        "outputId": "79d04e3a-130c-4318-853a-e6e946b8cded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Valid pairs: 4869\n"
          ]
        }
      ],
      "source": [
        "label_file = \"/content/mvsa_dataset/MVSA_Single/labelResultAll.txt\"\n",
        "data_dir = \"/content/mvsa_dataset/MVSA_Single/data\"\n",
        "\n",
        "valid_samples = []\n",
        "\n",
        "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        name, label = parts[0], parts[1]\n",
        "        img_path = os.path.join(data_dir, f\"{name}.jpg\")\n",
        "        txt_path = os.path.join(data_dir, f\"{name}.txt\")\n",
        "\n",
        "        if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "            valid_samples.append((name, label))\n",
        "\n",
        "print(f\"✅ Valid pairs: {len(valid_samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzyXk3nLjdt5",
        "outputId": "8d441ad2-2e37-4d69-a9b8-71e1e2c5acf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Valid pairs found: 4869\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "label_file = \"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\n",
        "data_dir = \"/content/MVSA_Single/MVSA_Single\"\n",
        "\n",
        "image_ext = \".jpg\"\n",
        "valid_samples = []\n",
        "\n",
        "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        name, label = parts[0], parts[1]\n",
        "\n",
        "        img_path = os.path.join(data_dir, \"data\", f\"{name}{image_ext}\")\n",
        "        txt_path = os.path.join(data_dir, \"data\", f\"{name}.txt\")\n",
        "\n",
        "        if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "            valid_samples.append((name, label))\n",
        "\n",
        "print(f\"✅ Valid pairs found: {len(valid_samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHnT4Wx0jicg"
      },
      "outputs": [],
      "source": [
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, samples, data_dir, transform=None, tokenizer=None):\n",
        "        self.samples = samples\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label_str = self.samples[idx]\n",
        "        label_str = label_str.split(',')[0].strip().lower()\n",
        "        label = self.label_map.get(label_str, 1)  # Default to neutral if unknown\n",
        "\n",
        "        img_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.jpg\")\n",
        "        txt_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.txt\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        text = open(txt_path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_inputs = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "        return image, text_inputs[\"input_ids\"].squeeze(0), text_inputs[\"attention_mask\"].squeeze(0), torch.tensor(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO_Etxgejm46"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = MVSADataset(valid_samples, data_dir=\"/content/MVSA_Single/MVSA_Single\", transform=transform, tokenizer=tokenizer)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcXtwyWHjpbN",
        "outputId": "b3bc2cc1-373f-408b-912a-6412e6a87acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Valid pairs: 0\n"
          ]
        }
      ],
      "source": [
        "label_file = \"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\n",
        "data_dir = \"/content/MVSA_Single/data\"\n",
        "\n",
        "valid_samples = []\n",
        "\n",
        "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        name, label = parts[0], parts[1]\n",
        "        img_path = os.path.join(data_dir, f\"{name}.jpg\")\n",
        "        txt_path = os.path.join(data_dir, f\"{name}.txt\")\n",
        "\n",
        "        if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "            valid_samples.append((name, label))\n",
        "\n",
        "print(f\"✅ Valid pairs: {len(valid_samples)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH6t4C-Uj12N",
        "outputId": "f5471b7d-d1d4-4487-fabf-240fb5ec7a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] ID\ttext,image\n",
            "[1] 1\tneutral,positive\n",
            "[2] 2\tneutral,positive\n",
            "[3] 3\tneutral,positive\n",
            "[4] 4\tpositive,positive\n",
            "[5] 5\tpositive,positive\n",
            "[6] 6\tpositive,positive\n",
            "[7] 7\tpositive,positive\n",
            "[8] 8\tneutral,positive\n",
            "[9] 9\tpositive,positive\n"
          ]
        }
      ],
      "source": [
        "# Show a few lines from the label file\n",
        "label_file_path = \"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\n",
        "\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(f\"[{i}] {line.strip()}\")\n",
        "        if i == 9:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR3JDX4cj81X",
        "outputId": "dbc67315-c3e9-489b-8291-24e14a3d73c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image exists: True\n",
            "Text exists: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "sample_id = \"123\"  # <-- Replace this with a real number from step 1 output\n",
        "\n",
        "img_path = f\"/content/MVSA_Single/MVSA_Single/data/{sample_id}.jpg\"\n",
        "txt_path = f\"/content/MVSA_Single/MVSA_Single/data/{sample_id}.txt\"\n",
        "\n",
        "print(\"Image exists:\", os.path.exists(img_path))\n",
        "print(\"Text exists:\", os.path.exists(txt_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjIPoVNLkAp_",
        "outputId": "a62ade7f-2d22-491c-d538-c47a7073d47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample files:\n",
            "['2785.jpg', '416.jpg', '2615.jpg', '3292.txt', '3829.txt', '2574.jpg', '230.txt', '1366.txt', '750.txt', '2869.jpg']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/content/MVSA_Single/MVSA_Single/data\"\n",
        "files = os.listdir(data_path)\n",
        "\n",
        "print(\"Sample files:\")\n",
        "print(files[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kyd7TWkkDMT"
      },
      "outputs": [],
      "source": [
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, label_file, transform=None, tokenizer=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_file = label_file\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.samples = []\n",
        "        self.label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "        with open(self.label_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "\n",
        "                file_id = parts[0]\n",
        "                label_str = parts[1].split(',')[0].strip().lower()  # handle things like \"positive,neutral\"\n",
        "\n",
        "                img_path = os.path.join(self.data_dir, f\"{file_id}.jpg\")\n",
        "                text_path = os.path.join(self.data_dir, f\"{file_id}.txt\")\n",
        "\n",
        "                if os.path.exists(img_path) and os.path.exists(text_path):\n",
        "                    label = self.label_map.get(label_str)\n",
        "                    if label is not None:\n",
        "                        self.samples.append((img_path, text_path, label))\n",
        "\n",
        "        print(f\"✅ Total valid samples loaded: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text_path, label = self.samples[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        text = open(text_path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            text_inputs[\"input_ids\"].squeeze(0),\n",
        "            text_inputs[\"attention_mask\"].squeeze(0),\n",
        "            label\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZO4RlOekPET",
        "outputId": "f0e2c977-2006-4be1-8a75-e2c61840a315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Total valid samples loaded: 4869\n"
          ]
        }
      ],
      "source": [
        "dataset = MVSADataset(\n",
        "    data_dir=\"/content/MVSA_Single/MVSA_Single/data\",\n",
        "    label_file=\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\",\n",
        "    transform=your_transforms,\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY4hIDI3kQ-Y"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyVeFzFkkceb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "\n",
        "        # Image feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, 256)\n",
        "\n",
        "        # Text feature extractor\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_fc = nn.Linear(self.bert.config.hidden_size, 256)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 3)  # 3 sentiment classes\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        img_feat = self.cnn(image)\n",
        "        text_feat = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
        "        text_feat = self.text_fc(text_feat)\n",
        "\n",
        "        combined = torch.cat((img_feat, text_feat), dim=1)\n",
        "        out = self.classifier(combined)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgjkXXfrkein"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hi9KJqEkjiq"
      },
      "outputs": [],
      "source": [
        "# Safe text loading with fallback encoding\n",
        "try:\n",
        "    with open(\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "except UnicodeDecodeError:\n",
        "    with open(\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"latin1\") as f:\n",
        "        text = f.read().strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyRkooUmk4Hb"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "    img_name, sentiment = self.samples[idx]\n",
        "    img_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.jpg\")\n",
        "    text_path = os.path.join(self.data_dir, \"data\", f\"{img_name}.txt\")\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Try loading text with utf-8, fallback to latin1 if it fails\n",
        "    try:\n",
        "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(text_path, \"r\", encoding=\"latin1\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "    # Apply transforms\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = self.tokenizer(\n",
        "        text, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    label = self.label2idx[sentiment]\n",
        "\n",
        "    return (\n",
        "        image,\n",
        "        tokens[\"input_ids\"].squeeze(0),\n",
        "        tokens[\"attention_mask\"].squeeze(0),\n",
        "        torch.tensor(label),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_aJAwTSlAho"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc86vJ77lXIm"
      },
      "outputs": [],
      "source": [
        "from chardet import detect  # Optional: for auto-encoding detection (needs `pip install chardet`)\n",
        "\n",
        "def __getitem__(self, idx):\n",
        "    img_path, text_path, label = self.valid_pairs[idx]\n",
        "\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    try:\n",
        "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()\n",
        "    except UnicodeDecodeError:\n",
        "        # Option 1: Try ISO-8859-1 (Latin-1)\n",
        "        with open(text_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        # Option 2 (better): Use chardet to detect encoding\n",
        "        # with open(text_path, \"rb\") as f:\n",
        "        #     raw = f.read()\n",
        "        #     encoding = detect(raw)[\"encoding\"]\n",
        "        #     text = raw.decode(encoding).strip()\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "    attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "    return image, input_ids, attention_mask, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q4CzAHVlf4Z"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)  # Adjust learning rate if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1KcEw_NlrEm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    with open(\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().strip()\n",
        "except UnicodeDecodeError:\n",
        "    with open(\"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\", \"r\", encoding=\"latin-1\") as f:\n",
        "        text = f.read().strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuwCSGlEmRU9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, data_dir, labels_dict, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.labels_dict = labels_dict\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = self._get_valid_samples()\n",
        "\n",
        "    def _get_valid_samples(self):\n",
        "        samples = []\n",
        "        for key, label in self.labels_dict.items():\n",
        "            img_path = os.path.join(self.data_dir, f\"{key}.jpg\")\n",
        "            txt_path = os.path.join(self.data_dir, f\"{key}.txt\")\n",
        "            if os.path.isfile(img_path) and os.path.isfile(txt_path):\n",
        "                samples.append((img_path, txt_path, label))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, txt_path, label = self.samples[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Load text (handle encoding errors)\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                  truncation=True, max_length=self.max_length)\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        # Map labels to int\n",
        "        label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "        label = label_map[label.lower()]\n",
        "\n",
        "        return image, input_ids, attention_mask, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liWvHDUnmon7"
      },
      "outputs": [],
      "source": [
        "def load_labels(label_file_path):\n",
        "    labels_dict = {}\n",
        "    with open(label_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                key = parts[0]\n",
        "                label = parts[1].lower()\n",
        "                if label in [\"positive\", \"neutral\", \"negative\"]:\n",
        "                    labels_dict[key] = label\n",
        "    return labels_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDZX1LxMmsXW",
        "outputId": "afc1be96-1161-4891-e08c-54cbcc1dc534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images & Text files: 9738\n",
            "ID\ttext,image\r\n",
            "1\tneutral,positive\r\n",
            "2\tneutral,positive\r\n",
            "3\tneutral,positive\r\n",
            "4\tpositive,positive\r\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Images & Text files:\", len(os.listdir(\"/content/MVSA_Single/MVSA_Single/data\")))\n",
        "!head -n 5 /content/MVSA_Single/MVSA_Single/labelResultAll.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8y7gC_Rm1BL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_labels(label_file_path):\n",
        "    labels_dict = {}\n",
        "    with open(label_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')  # Tab-separated\n",
        "        next(reader)  # Skip the header line\n",
        "        for row in reader:\n",
        "            if len(row) == 2:\n",
        "                sample_id = row[0].strip()\n",
        "                label_str = row[1].strip()  # e.g. \"neutral,positive\"\n",
        "                text_label = label_str.split(',')[0].lower()  # Get the text label only\n",
        "                if text_label in [\"positive\", \"neutral\", \"negative\"]:\n",
        "                    labels_dict[sample_id] = text_label\n",
        "    return labels_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUsamEzBm_T2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def load_labels(label_file_path):\n",
        "    labels_dict = {}\n",
        "    with open(label_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if len(row) == 2:\n",
        "                sample_id = row[0].strip()\n",
        "                label_str = row[1].strip()\n",
        "                text_label = label_str.split(',')[0].lower()  # Use only text label\n",
        "                if text_label in [\"positive\", \"neutral\", \"negative\"]:\n",
        "                    labels_dict[sample_id] = text_label\n",
        "    return labels_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zc939MtnGPR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, data_dir, labels_dict, transform=None, tokenizer=None, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.labels_dict = labels_dict\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label2id = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "        for id_, label in labels_dict.items():\n",
        "            img_path = os.path.join(data_dir, \"data\", f\"{id_}.jpg\")\n",
        "            txt_path = os.path.join(data_dir, \"data\", f\"{id_}.txt\")\n",
        "            if os.path.exists(img_path) and os.path.exists(txt_path):\n",
        "                self.samples.append((img_path, txt_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, txt_path, label = self.samples[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            encoding[\"input_ids\"].squeeze(0),\n",
        "            encoding[\"attention_mask\"].squeeze(0),\n",
        "            torch.tensor(self.label2id[label])\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nP7vREinIwB",
        "outputId": "bb947c96-5ee7-4e13-f6c8-dc0dfb0c4670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 4869\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torchvision import transforms\n",
        "\n",
        "# Paths\n",
        "label_file_path = \"/content/MVSA_Single/MVSA_Single/labelResultAll.txt\"\n",
        "data_dir = \"/content/MVSA_Single/MVSA_Single\"\n",
        "\n",
        "# Load labels\n",
        "labels_dict = load_labels(label_file_path)\n",
        "\n",
        "# Tokenizer & Transforms\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = MVSADataset(\n",
        "    data_dir=data_dir,\n",
        "    labels_dict=labels_dict,\n",
        "    transform=transform,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Total samples: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zfPDrEnLC1",
        "outputId": "ee2b3923-a92f-4206-cff2-897757d1313c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 3895, Val: 974\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Split dataset (80% train, 20% val)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=16)\n",
        "\n",
        "print(f\"Train: {len(train_set)}, Val: {len(val_set)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0hCZ7u2nRYr"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultiModalSentimentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalSentimentModel, self).__init__()\n",
        "        # Image encoder\n",
        "        self.cnn = resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Output 512-d features\n",
        "\n",
        "        # Text encoder\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.text_fc = nn.Linear(self.bert.config.hidden_size, 256)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 3)  # 3 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        img_feat = self.cnn(image)\n",
        "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_feat = self.text_fc(text_outputs.pooler_output)\n",
        "\n",
        "        combined = torch.cat((img_feat, text_feat), dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P4-1nCGnVe7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiModalSentimentModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "num_epochs = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3g5T3mxnXpD",
        "outputId": "75ce57c3-fc8b-4c59-89fe-ed09ba9b5999"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]: 100%|██████████| 244/244 [01:55<00:00,  2.11it/s, acc=53.5, loss=1.27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 72.07%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/5]: 100%|██████████| 244/244 [01:53<00:00,  2.14it/s, acc=76.2, loss=1.22]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 73.41%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/5]: 100%|██████████| 244/244 [01:53<00:00,  2.15it/s, acc=85.7, loss=0.282]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 76.18%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/5]: 100%|██████████| 244/244 [01:53<00:00,  2.15it/s, acc=92, loss=0.404]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 74.85%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [5/5]: 100%|██████████| 244/244 [01:52<00:00,  2.16it/s, acc=96.4, loss=0.056]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 74.74%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    for images, input_ids, attention_mask, labels in loop:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item(), acc=100*correct/len(train_loader.dataset))\n",
        "\n",
        "    # Validation after each epoch\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * val_correct / len(val_loader.dataset)\n",
        "    print(f\"Validation Accuracy: {val_acc:.2f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0oYwzCjnaGG",
        "outputId": "57dd35c4-10e4-43c5-ea19-e67e8f3955a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeeukHQhwo_E"
      },
      "outputs": [],
      "source": [
        "# Define the path in Google Drive\n",
        "model_save_path = \"/content/drive/MyDrive/Colab Notebooks/multimodal_sentiment_model.pth\"\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvWyXCvIzT9k"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load tokenizer (same as used during training)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define label mapping (adjust if your order is different)\n",
        "idx2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "# Prediction function\n",
        "def predict(image_path, text, model, transform, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Image preprocessing\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0).to(device)  # Add batch dim\n",
        "\n",
        "    # Text preprocessing\n",
        "    encoding = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image, input_ids, attention_mask)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    return idx2label[pred_label], probs.cpu().numpy().squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRllKBgHudzA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKjXkJRpRSli"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDeqsfCmRjk"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultimodalSentimentModel(nn.Module):\n",
        "    def __init__(self, text_model_name='bert-base-uncased', num_classes=3):\n",
        "        super(MultimodalSentimentModel, self).__init__()\n",
        "\n",
        "        # Text branch (BERT)\n",
        "        self.text_model = BertModel.from_pretrained(text_model_name)\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 256)\n",
        "\n",
        "        # Image branch (ResNet)\n",
        "        resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        resnet.fc = nn.Identity()  # remove final FC layer\n",
        "        self.image_model = resnet\n",
        "        self.image_fc = nn.Linear(512, 256)\n",
        "\n",
        "        # Combined\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        # Text encoding\n",
        "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_outputs.pooler_output)\n",
        "\n",
        "        # Image encoding\n",
        "        image_features = self.image_model(image)\n",
        "        image_features = self.image_fc(image_features)\n",
        "\n",
        "        # Concatenate\n",
        "        combined = torch.cat((image_features, text_features), dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "lKoHaVY0mnfv",
        "outputId": "4a88224d-20e5-4e57-f387-e3f6336b893d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torchvision' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0ea870e5fc4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ],
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2WjlsvVmvJS",
        "outputId": "09626c55-40c9-4fd6-ce64-b4d051e5f3a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 125MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "resnet = torchvision.models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dd48wgAnP3f"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultimodalSentimentModel(nn.Module):\n",
        "    def __init__(self, text_model_name='bert-base-uncased', num_classes=3):\n",
        "        super(MultimodalSentimentModel, self).__init__()\n",
        "\n",
        "        # Text branch (BERT)\n",
        "        self.text_model = BertModel.from_pretrained(text_model_name)\n",
        "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 256)\n",
        "\n",
        "        # Image branch (ResNet)\n",
        "        resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        resnet.fc = nn.Identity()  # remove final FC layer\n",
        "        self.image_model = resnet\n",
        "        self.image_fc = nn.Linear(512, 256)\n",
        "\n",
        "        # Combined\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        # Text encoding\n",
        "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = self.text_fc(text_outputs.pooler_output)\n",
        "\n",
        "        # Image encoding\n",
        "        image_features = self.image_model(image)\n",
        "        image_features = self.image_fc(image_features)\n",
        "\n",
        "        # Concatenate\n",
        "        combined = torch.cat((image_features, text_features), dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nG_4gWmnmd5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalSentimentModel(num_classes=3).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUVt9AKlnra9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UwhAyNlnoNuD",
        "outputId": "9c8e03ef-94fb-472d-a4d8-50bce82ba768"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6f063c6210c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assuming 'dataset' is already created using your custom Dataset class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Assuming 'dataset' is already created using your custom Dataset class\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    for images, input_ids, attention_mask, labels in loop:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    train_f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Train F1 Score: {train_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xPQb77PnoUxQ",
        "outputId": "1716db3f-ae96-45dd-daba-ddbd1446157d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labels.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-aebe26a85eb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Label mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MVSA_Single/MVSA_Single/labels.csv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/MVSA_Single/MVSA_Single/data\"\n",
        "labels_file = \"/content/MVSA_Single/MVSA_Single/labels.csv\"\n",
        "\n",
        "# Load labels\n",
        "df = pd.read_csv(labels_file)\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "\n",
        "# Define Dataset\n",
        "class MVSADataset(Dataset):\n",
        "    def __init__(self, dataframe, data_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        id = str(row[\"ID\"])\n",
        "\n",
        "        img_path = os.path.join(self.data_dir, f\"{id}.jpg\")\n",
        "        text_path = os.path.join(self.data_dir, f\"{id}.txt\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        try:\n",
        "            with open(text_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read().strip()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(text_path, 'r', encoding='latin-1') as f:\n",
        "                text = f.read().strip()\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Use the first label only (image label)\n",
        "        label = label_map[row[\"image\"]]\n",
        "\n",
        "        return image, input_ids, attention_mask, torch.tensor(label)\n",
        "\n",
        "# Image Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "dataset = MVSADataset(df, data_dir, transform)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=16)\n",
        "\n",
        "# Dummy model (replace this with your actual model)\n",
        "import torch.nn as nn\n",
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.image_branch = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3 * 224 * 224, 128)\n",
        "        )\n",
        "        self.text_branch = nn.Sequential(\n",
        "            nn.Linear(768, 128)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        img_feat = self.image_branch(image)\n",
        "        text_feat = self.text_branch(input_ids.float())  # Dummy placeholder\n",
        "        combined = torch.cat((img_feat, text_feat), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# Train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DummyModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_preds, total_labels = [], []\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/5]\")\n",
        "\n",
        "    for images, input_ids, attention_mask, labels in loop:\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        total_preds.extend(preds.cpu().numpy())\n",
        "        total_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    acc = accuracy_score(total_labels, total_preds)\n",
        "    f1 = f1_score(total_labels, total_preds, average='weighted')\n",
        "    print(f\"Train Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "wYiZ7tlLpKD0",
        "outputId": "87d1c0f2-1e05-406f-c483-8de47f8ba179"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6f063c6210c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assuming 'dataset' is already created using your custom Dataset class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Assuming 'dataset' is already created using your custom Dataset class\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Optimizer & Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    for images, input_ids, attention_mask, labels in loop:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    train_f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Train F1 Score: {train_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQmrQYzepOSW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0024bad4a6434cba8f54340032c3ed7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215fe825ad454e92a3164cf3d922b5ec",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_305a55df466e4165ae88306c2ec829c7",
            "value": 466062
          }
        },
        "026c7e64b78449bdaf38a991e92cafa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9979a8604da64a3f91e4033277c30624",
            "placeholder": "​",
            "style": "IPY_MODEL_d0347be7eb3744b782c48afd316e17a5",
            "value": "vocab.txt: 100%"
          }
        },
        "06b35e8dcb244e2482197b2e1f3118b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0799a10001a848cdb6a582738bf2f87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10d578fca57d42af974641984f2844d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d8ac99693f432d8505098e7c8a2744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12f6878dd2a3402c8296fc78b85dba5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a22f3c89fa1470086ced18502d5288a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce3bf84715540fbb84e9f406bfb2f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4f66b55cd34742bf6130100f79998a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12f6878dd2a3402c8296fc78b85dba5e",
            "placeholder": "​",
            "style": "IPY_MODEL_88f7f2570314435e80b898247d1985d6",
            "value": " 440M/440M [00:01&lt;00:00, 314MB/s]"
          }
        },
        "1fb16bed599e4e15bcfeae4b5d2e07d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4e4f8e56cb499e8c1a9c7494e77256",
            "placeholder": "​",
            "style": "IPY_MODEL_3b666d2db40d45509178833f80672ca8",
            "value": "config.json: 100%"
          }
        },
        "1ffdbbbc10ab47c3a480d728dce49963": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215fe825ad454e92a3164cf3d922b5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2452d1cf1fc84d388f3db1482438a6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fb16bed599e4e15bcfeae4b5d2e07d2",
              "IPY_MODEL_47bbad65306c4f31a33b003be8b30a94",
              "IPY_MODEL_f5bc3aee3cd44912a40ea5916cd5d9b2"
            ],
            "layout": "IPY_MODEL_1ffdbbbc10ab47c3a480d728dce49963"
          }
        },
        "2fa222f42b2c4eeeb01971d60bd3f5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305a55df466e4165ae88306c2ec829c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "346e6ade3e044a52ab8ef498ffea4cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76dbeac688ba443fa7966047d4384e69",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0799a10001a848cdb6a582738bf2f87a",
            "value": 48
          }
        },
        "3b666d2db40d45509178833f80672ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47bbad65306c4f31a33b003be8b30a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa222f42b2c4eeeb01971d60bd3f5e6",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ce3bf84715540fbb84e9f406bfb2f4b",
            "value": 570
          }
        },
        "4a73a6eca7144db0a4d38f99c84fcec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05d2309ffd84d0398f85fa18b19a1be",
            "placeholder": "​",
            "style": "IPY_MODEL_10d578fca57d42af974641984f2844d4",
            "value": " 232k/232k [00:00&lt;00:00, 3.23MB/s]"
          }
        },
        "4f155551fb7149f9ab7a6062194f788d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ce6716ed6d41579a0c2e68c7c5a36f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f7f823bb52442f92be323b2666b2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c0b115c7f5a45f69f486701b85ec58d",
              "IPY_MODEL_346e6ade3e044a52ab8ef498ffea4cec",
              "IPY_MODEL_61fe7ef63ee44d999fa43ee0edd4a93a"
            ],
            "layout": "IPY_MODEL_4f155551fb7149f9ab7a6062194f788d"
          }
        },
        "56713b567d6b45598895d5fce85c34b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583e89f516494a748c79ba2b45273f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c009b7f38e24e05b60727af673f94d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61fe7ef63ee44d999fa43ee0edd4a93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56713b567d6b45598895d5fce85c34b7",
            "placeholder": "​",
            "style": "IPY_MODEL_f47ffef2380640b6942a6c56cd61c76d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.99kB/s]"
          }
        },
        "62330837cb134a81b9cb25142b691f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67700240e5d44f2da42edeca87355e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfe929622024f25960f359251e2f23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f868c556e1747859b06a9a325f2e9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_026c7e64b78449bdaf38a991e92cafa6",
              "IPY_MODEL_9992d567c5b7443fa911f66e1f56051d",
              "IPY_MODEL_4a73a6eca7144db0a4d38f99c84fcec7"
            ],
            "layout": "IPY_MODEL_06b35e8dcb244e2482197b2e1f3118b5"
          }
        },
        "724d840315dc4af5a4c55ab097cf6b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c3d6f4f8b624c46bc7f273e33f2d96a",
              "IPY_MODEL_ae8c20b3c3a04d4d94612ccb601fdb51",
              "IPY_MODEL_1e4f66b55cd34742bf6130100f79998a"
            ],
            "layout": "IPY_MODEL_8b467da28cd1413cb8a523c63ce858e0"
          }
        },
        "76dbeac688ba443fa7966047d4384e69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba3afdb66ad40f983188c2f7ebae2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e678a9a6ff6c4cea83ac4235bb9b9482",
            "placeholder": "​",
            "style": "IPY_MODEL_12d8ac99693f432d8505098e7c8a2744",
            "value": " 466k/466k [00:00&lt;00:00, 3.14MB/s]"
          }
        },
        "7c0b115c7f5a45f69f486701b85ec58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_583e89f516494a748c79ba2b45273f0f",
            "placeholder": "​",
            "style": "IPY_MODEL_1a22f3c89fa1470086ced18502d5288a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7e4e4f8e56cb499e8c1a9c7494e77256": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a207a1c49247c79746977ec3f20b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67700240e5d44f2da42edeca87355e5b",
            "placeholder": "​",
            "style": "IPY_MODEL_6dfe929622024f25960f359251e2f23b",
            "value": "tokenizer.json: 100%"
          }
        },
        "83c8d64f031f493aa3fc964cd08dccea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88f7f2570314435e80b898247d1985d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b467da28cd1413cb8a523c63ce858e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3d6f4f8b624c46bc7f273e33f2d96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9208d2fc41b4d5e850d5a0ef0f8052d",
            "placeholder": "​",
            "style": "IPY_MODEL_f5abcc4650254c799c01b5cbd3d75e85",
            "value": "model.safetensors: 100%"
          }
        },
        "9979a8604da64a3f91e4033277c30624": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9992d567c5b7443fa911f66e1f56051d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbdc7f2e322d41bc9319ded17ac7b02e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c8d64f031f493aa3fc964cd08dccea",
            "value": 231508
          }
        },
        "a05d2309ffd84d0398f85fa18b19a1be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae8c20b3c3a04d4d94612ccb601fdb51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb261c10d2eb4caf9c21188081ce65e4",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c009b7f38e24e05b60727af673f94d7",
            "value": 440449768
          }
        },
        "b4f5cf6f1f83416382c11625dcc27335": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb261c10d2eb4caf9c21188081ce65e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbdc7f2e322d41bc9319ded17ac7b02e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0347be7eb3744b782c48afd316e17a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9208d2fc41b4d5e850d5a0ef0f8052d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e678a9a6ff6c4cea83ac4235bb9b9482": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47ffef2380640b6942a6c56cd61c76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5abcc4650254c799c01b5cbd3d75e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5bc3aee3cd44912a40ea5916cd5d9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62330837cb134a81b9cb25142b691f66",
            "placeholder": "​",
            "style": "IPY_MODEL_b4f5cf6f1f83416382c11625dcc27335",
            "value": " 570/570 [00:00&lt;00:00, 54.7kB/s]"
          }
        },
        "ff424c5d92fb4d1fb02775d536c7d072": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83a207a1c49247c79746977ec3f20b44",
              "IPY_MODEL_0024bad4a6434cba8f54340032c3ed7e",
              "IPY_MODEL_7ba3afdb66ad40f983188c2f7ebae2b3"
            ],
            "layout": "IPY_MODEL_50ce6716ed6d41579a0c2e68c7c5a36f"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
